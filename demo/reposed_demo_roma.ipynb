{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kocurvik/mdrp/blob/main/demo/reposed_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8RsHkYFPfe3"
      },
      "source": [
        "# RePosed Demo - Roma\n",
        "\n",
        "This is a demo for the paper \"RePoseD: Efficient Relative Pose Estimation With Known Depth Information\" ICCV 2025 (oral) paper. DOI: TBA. Pre-print available on Arxiv: [2501.07742](https://arxiv.org/abs/2501.07742).\n",
        "\n",
        "Code available in our [repo](https://github.com/kocurvik/mdrp). For more visual results see our [Project Page](https://kocurvik.github.io/reposed/).\n",
        "\n",
        "The demo shows how to use the proposed solvers with MoGe and LightGlue to obtain the relative pose of two cameras and even merge the two MDE point clouds.\n",
        "\n",
        "## Installing MoGe and LightGlue\n",
        "\n",
        "First we install MoGe and Lightglue using their official repositories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-aWcR_kEz8b",
        "outputId": "8f82f646-86a0-42df-93bf-bbc6b5dedcf5"
      },
      "outputs": [],
      "source": [
        "# Install MoGe for depth estimation\n",
        "!pip install git+https://github.com/microsoft/MoGe.git\n",
        "# PR version, probably wont work now\n",
        "!pip install https://github.com/kocurvik/mdrp/raw/refs/heads/main/demo/poselib-2.0.5-cp312-cp312-linux_x86_64.whl\n",
        "# Install Roma matching\n",
        "!pip install romatch[fused-local-corr]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKnZgqooOegR"
      },
      "source": [
        "### Installing PoseLib with Solvers\n",
        "\n",
        "Next we install PoseLib. To use this demo you can use the precompiled wheel. However, if you use a different python config (OS/python version) you may need to compile the C++ code yourself using (this will take few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8OSkbfSFv2U",
        "outputId": "b8948746-d571-4641-985a-19bc17c5cc14"
      },
      "outputs": [],
      "source": [
        "\n",
        "### If you need to compile PoseLib in Colab yourself then use this:\n",
        "#!git clone https://github.com/kocurvik/PoseLib\n",
        "#%cd PoseLib\n",
        "#!git checkout pr-mdrp\n",
        "#!pip install pybind11_stubs\n",
        "#!apt-get install libeigen3-dev\n",
        "#!python setup.py install\n",
        "\n",
        "### Outside of Google Colab you can try installing with:\n",
        "#!pip install git+https://github.com/kocurvik/PoseLib@pr-mdrp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLApGttpQjYQ"
      },
      "source": [
        "## Depth Estimation and Matching\n",
        "\n",
        "We first process two images with MoGe and SuperPoint+LightGlue. We start by loading the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxgcXreZ8XhG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# from moge.model.v1 import MoGeModel\n",
        "from moge.model.v2 import MoGeModel\n",
        "from romatch import roma_indoor\n",
        "\n",
        "import poselib\n",
        "\n",
        "mde_model = MoGeModel.from_pretrained(\"Ruicheng/moge-2-vitl-normal\").cuda()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "roma_model = roma_indoor(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2hp_re4AaEY"
      },
      "source": [
        "Next we load the images. You can use the provided example images. You can load your own images here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn1vh_XaAZrv",
        "outputId": "c4d3cab0-5504-466a-da8f-4b9110e02743"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/kocurvik/mdrp/refs/heads/main/demo/images/249.png\n",
        "!wget https://raw.githubusercontent.com/kocurvik/mdrp/refs/heads/main/demo/images/450.png\n",
        "im1 = \"249.png\"\n",
        "im2 = \"450.png\"\n",
        "cv_image1 = cv2.cvtColor(cv2.imread(im1), cv2.COLOR_BGR2RGB)\n",
        "cv_image2 = cv2.cvtColor(cv2.imread(im2), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# These images demonstrate why estimateting the scale of MDEs is important.\n",
        "# !wget https://raw.githubusercontent.com/kocurvik/mdrp/refs/heads/main/demo/images/toy_car_1.jpg\n",
        "# !wget https://raw.githubusercontent.com/kocurvik/mdrp/refs/heads/main/demo/images/toy_car_2.jpg\n",
        "# cv_image1 = cv2.cvtColor(cv2.imread(\"toy_car_1.jpg\"), cv2.COLOR_BGR2RGB)\n",
        "# cv_image2 = cv2.cvtColor(cv2.imread(\"toy_car_2.jpg\"), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# !wget https://raw.githubusercontent.com/kocurvik/mdrp/refs/heads/main/demo/images/still_1.jpg\n",
        "# !wget https://raw.githubusercontent.com/kocurvik/mdrp/refs/heads/main/demo/images/still_2.jpg\n",
        "# cv_image1 = cv2.cvtColor(cv2.imread(\"still_1.jpg\"), cv2.COLOR_BGR2RGB)\n",
        "# cv_image2 = cv2.cvtColor(cv2.imread(\"still_2.jpg\"), cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO32oRMvBQSq"
      },
      "source": [
        "Then we run inference using the networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6W56RbcLQipn",
        "outputId": "5c0a68b7-1049-4f86-fa5c-232bf601235b"
      },
      "outputs": [],
      "source": [
        "# convert images to torch tensor\n",
        "image1 = torch.tensor(cv_image1 / 255, dtype=torch.float32, device='cuda').permute(2, 0, 1)\n",
        "image2 = torch.tensor(cv_image2 / 255, dtype=torch.float32, device='cuda').permute(2, 0, 1)\n",
        "\n",
        "# Infer Moge\n",
        "mde_out1 = mde_model.infer(image1)\n",
        "mde_out2 = mde_model.infer(image2)\n",
        "\n",
        "\n",
        "\n",
        "plt.title(\"Image 1\")\n",
        "plt.imshow(cv_image1)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Image 1 - depth\")\n",
        "plt.imshow(mde_out1['depth'].detach().cpu().numpy(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.title(\"Image 2\")\n",
        "plt.imshow(cv_image2)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(mde_out2['depth'].detach().cpu().numpy(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Extract Roma features\n",
        "warp, certainty = roma_model.match(im1, im2, device=device)\n",
        "# Sample matches for estimation\n",
        "matches, certainty = roma_model.sample(warp, certainty)\n",
        "\n",
        "H_A, W_A = cv_image1.shape[:2]\n",
        "H_B, W_B = cv_image2.shape[:2]\n",
        "# Convert to pixel coordinates (RoMa produces matches in [-1,1]x[-1,1])\n",
        "kptsA, kptsB = roma_model.to_pixel_coordinates(matches, H_A, W_A, H_B, W_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xoGzxnUBUWE"
      },
      "source": [
        "We set up a function to visualize the pointcloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EjtGVri8WuXs"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# code taken from: https://github.com/cvg/Hierarchical-Localization/blob/master/hloc/utils/viz_3d.py\n",
        "\n",
        "def init_figure(height: int = 800) -> go.Figure:\n",
        "    \"\"\"Initialize a 3D figure.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    axes = dict(\n",
        "        visible=False,\n",
        "        showbackground=False,\n",
        "        showgrid=False,\n",
        "        showline=False,\n",
        "        showticklabels=True,\n",
        "        autorange=True,\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        template=\"plotly_dark\",\n",
        "        height=height,\n",
        "        scene_camera=dict(\n",
        "            eye=dict(x=0., y=-.1, z=-2),\n",
        "            up=dict(x=0, y=-1., z=0),\n",
        "            projection=dict(type=\"orthographic\")),\n",
        "        scene=dict(\n",
        "            xaxis=axes,\n",
        "            yaxis=axes,\n",
        "            zaxis=axes,\n",
        "            aspectmode='data',\n",
        "            dragmode='orbit',\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"top\",\n",
        "            y=0.99,\n",
        "            xanchor=\"left\",\n",
        "            x=0.1\n",
        "        ),\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_points(\n",
        "        fig: go.Figure,\n",
        "        pts: np.ndarray,\n",
        "        color: str = 'rgba(255, 0, 0, 1)',\n",
        "        ps: int = 2,\n",
        "        colorscale: Optional[str] = None,\n",
        "        name: Optional[str] = None):\n",
        "    \"\"\"Plot a set of 3D points.\"\"\"\n",
        "    x, y, z = pts.T\n",
        "    tr = go.Scatter3d(\n",
        "        x=x, y=y, z=z, mode='markers', name=name, legendgroup=name,\n",
        "        marker=dict(\n",
        "            size=ps, color=color, line_width=0.0, colorscale=colorscale))\n",
        "    fig.add_trace(tr)\n",
        "\n",
        "\n",
        "def plot_interactive_pointcloud(xyz, rgb, subsampling_rate=0.05):\n",
        "  # plots a plotly pointcloud\n",
        "  # params:\n",
        "  # xyz - n x 3 - array with 3D coordinates of points\n",
        "  # rgb - n x 3 - array with RGB triplets in 0-255\n",
        "  n = xyz.shape[0]\n",
        "\n",
        "  l = np.random.rand(n) <= subsampling_rate\n",
        "  fig = init_figure()\n",
        "  plot_points(fig, xyz[l], color=rgb[l])\n",
        "  fig.show()\n",
        "\n",
        "# alternative viewer\n",
        "# !pip install open3d\n",
        "# import open3d as o3d\n",
        "# def plot_interactive_pointcloud(xyz, rgb):\n",
        "#     pcd = o3d.geometry.PointCloud()\n",
        "#     pcd.points = o3d.utility.Vector3dVector(xyz)\n",
        "#     pcd.colors = o3d.utility.Vector3dVector(rgb)  # Colors should be in [0,1]\n",
        "\n",
        "#     # Visualize using Plotly\n",
        "#     o3d.visualization.draw_plotly([pcd], point_sample_factor=0.05, window_name=\"Point Cloud Viewer\", width=800, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzbI8nOCu4I"
      },
      "source": [
        "Once depth and the intrinsics are available we can calculate the pointcloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b1VVHNgWY-OO"
      },
      "outputs": [],
      "source": [
        "def get_pointcloud(K_inv, depth_map, color_map):\n",
        "  H, W = depth_map.shape\n",
        "  u = np.arange(W)\n",
        "  v = np.arange(H)\n",
        "  uu, vv = np.meshgrid(u, v)\n",
        "\n",
        "  z = depth_map.flatten()\n",
        "  uu = uu.flatten()\n",
        "  vv = vv.flatten()\n",
        "  proj = np.vstack([uu, vv, np.ones_like(uu)])\n",
        "\n",
        "  flattened_colors = color_map.reshape([-1, 3])\n",
        "\n",
        "  return z[:, np.newaxis] * (K_inv @ proj).T, flattened_colors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUGMdmwmBc7s"
      },
      "source": [
        "### Running RePoseD\n",
        "\n",
        "Finally, we can run the poselib implementation of RePoseD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b0kHWUFZVsYu",
        "outputId": "11a7cfe6-474d-4bf0-e0c3-4570e064d86f"
      },
      "outputs": [],
      "source": [
        "# convert the matches to numpy\n",
        "points1 = kptsA.cpu().numpy()\n",
        "points2 = kptsB.cpu().numpy()\n",
        "\n",
        "# extract depths for each keypoint\n",
        "depth_map1 = mde_out1[\"depth\"].cpu().numpy()\n",
        "depth_map2 = mde_out2[\"depth\"].cpu().numpy()\n",
        "depths1 = depth_map1[points1[:, 1].astype(int), points1[:, 0].astype(int)]\n",
        "depths2 = depth_map2[points2[:, 1].astype(int), points2[:, 0].astype(int)]\n",
        "\n",
        "l = ~np.logical_and(np.isinf(depths1), np.isinf(depths2))\n",
        "\n",
        "pp1 = np.array([cv_image1.shape[1] / 2, cv_image1.shape[0] / 2])\n",
        "pp2 = np.array([cv_image2.shape[1] / 2, cv_image2.shape[0] / 2])\n",
        "\n",
        "ransac_dict = {'max_epipolar_error': 2.0, 'max_reproj_error': 16.0}\n",
        "# set this to true if you also want to estimate shit (calib case only)\n",
        "ransac_dict['estimate_shift'] = False\n",
        "\n",
        "# use this loss for better estimation\n",
        "bundle_dict = {'loss_type': 'TRUNCATED_CAUCHY'}\n",
        "\n",
        "h1, w1 = cv_image1.shape[:2]\n",
        "h2, w2 = cv_image2.shape[:2]\n",
        "\n",
        "# instrinsics1 = mde_out1['intrinsics'].cpu().numpy()\n",
        "# instrinsics2 = mde_out2['intrinsics'].cpu().numpy()\n",
        "\n",
        "# # MoGe also provides estimated focal length\n",
        "# f1, px1, py1 = w1*instrinsics1[0, 0], w1*instrinsics1[0, 2], h1*instrinsics1[1, 2]\n",
        "# f2, px2, py2 = w2*instrinsics2[0, 0], w2*instrinsics2[0, 2], h2*instrinsics2[1, 2]\n",
        "\n",
        "# ## Since focals are known we can use the calibrated solver.\n",
        "# camera1 = {'model': 'SIMPLE_PINHOLE', 'width': -1, 'height': -1, 'params': [f1, px1, py1]}\n",
        "# camera2 = {'model': 'SIMPLE_PINHOLE', 'width': -1, 'height': -1, 'params': [f2, px2, py2]}\n",
        "# pose, info = poselib.estimate_monodepth_pose(points1[l], points2[l], depths1[l], depths2[l], camera1, camera2, ransac_dict, bundle_dict)\n",
        "\n",
        "## In case focals are not known or you don't trust them you can use this code to\n",
        "## estimate them along with the pose with when the images are taken by a single\n",
        "## camera.\n",
        "image_pair, info = poselib.estimate_monodepth_shared_focal_pose(points1[l] - pp1, points2[l] - pp2, depths1[l], depths2[l], ransac_dict, bundle_dict)\n",
        "f1 = image_pair.camera1.focal()\n",
        "f2 = image_pair.camera2.focal()\n",
        "pose = image_pair.pose\n",
        "\n",
        "## If the images are captured by different cameras you can use this version.\n",
        "# image_pair, info = poselib.estimate_monodepth_varying_focal_pose(points1[l] - pp1, points2[l] - pp2, depths1[l], depths2[l], ransac_dict, bundle_dict)\n",
        "# f1 = image_pair.camera1.focal()\n",
        "# f2 = image_pair.camera2.focal()\n",
        "# pose = image_pair.pose\n",
        "\n",
        "print(info)\n",
        "\n",
        "K1 = np.array([[f1, 0, pp1[0]], [0, f1, pp1[1]],[0, 0, 1]])\n",
        "K2 = np.array([[f2, 0, pp2[0]], [0, f2, pp2[1]],[0, 0, 1]])\n",
        "K1_inv = np.linalg.inv(K1)\n",
        "K2_inv = np.linalg.inv(K2)\n",
        "\n",
        "xyz1, rgb1 = get_pointcloud(K1_inv, depth_map1, cv_image1)\n",
        "xyz2, rgb2 = get_pointcloud(K2_inv, depth_map2, cv_image2)\n",
        "\n",
        "\n",
        "# xyz1_in2 = (image_pair.pose.R @ xyz1.T).T + image_pair.pose.t\n",
        "xyz1_in2 = (1/pose.scale) * ((pose.R @ xyz1.T).T + pose.t)\n",
        "\n",
        "print(\"Estimated scale: \", pose.scale)\n",
        "\n",
        "xyz_merged = np.vstack([xyz1_in2, xyz2])\n",
        "rgb_merged = np.vstack([rgb1, rgb2])\n",
        "\n",
        "plot_interactive_pointcloud(xyz_merged, rgb_merged)\n",
        "\n",
        "# if you want a nicer pointcloud you can use\n",
        "# note that this may cause lag or make Google Colab completely unresponsive\n",
        "# plot_interactive_pointcloud(xyz_merged, rgb_merged, subsampling_rate=1.0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
